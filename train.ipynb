{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run the training script below, run the `train.py` file with hyperparameters and arguments defined. `data_path` and `file_name` arguments are needed. Hyparparameters like learning rate (`lr`), epoch (`epoch`), batch size (`batch_size`), activation function (`activation_func`) can be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xl/k93g9t_j53bbn9xxy24fh8v80000gn/T/ipykernel_6107/740113558.py:4: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data ... \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=135'>136</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m''' Prepare training data'''\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=136'>137</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPreparing training data ... \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=137'>138</a>\u001b[0m train_data_loader \u001b[39m=\u001b[39m common_utils\u001b[39m.\u001b[39mDeepSupervisionDataLoader(dataset_dir\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\n\u001b[1;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=138'>139</a>\u001b[0m     args\u001b[39m.\u001b[39;49mdata_path, \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m), listfile\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(args\u001b[39m.\u001b[39mdata_path, \u001b[39m'\u001b[39m\u001b[39mtrain_listfile.csv\u001b[39m\u001b[39m'\u001b[39m), small_part\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39msmall_part)\n\u001b[1;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=139'>140</a>\u001b[0m val_data_loader \u001b[39m=\u001b[39m common_utils\u001b[39m.\u001b[39mDeepSupervisionDataLoader(dataset_dir\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=140'>141</a>\u001b[0m     args\u001b[39m.\u001b[39mdata_path, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m), listfile\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(args\u001b[39m.\u001b[39mdata_path, \u001b[39m'\u001b[39m\u001b[39mval_listfile.csv\u001b[39m\u001b[39m'\u001b[39m), small_part\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39msmall_part)\n\u001b[1;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=141'>142</a>\u001b[0m discretizer \u001b[39m=\u001b[39m Discretizer(timestep\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, store_masks\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=142'>143</a>\u001b[0m                         impute_strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mprevious\u001b[39m\u001b[39m'\u001b[39m, start_time\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mzero\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m<frozen posixpath>:76\u001b[0m, in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import imp\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "RANDOM_SEED = 12345\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "\n",
    "from utils import utils\n",
    "from utils.readers import DecompensationReader\n",
    "from utils.preprocessing import Discretizer, Normalizer\n",
    "from utils import metrics\n",
    "from utils import common_utils\n",
    "from model import AdaCare\n",
    "\n",
    "def parse_arguments(parser):\n",
    "    parser.add_argument('--test_mode', type=int, default=0, help='Test SA-CRNN on MIMIC-III dataset')\n",
    "    parser.add_argument('--data_path', type=str, metavar='<data_path>', help='The path to the MIMIC-III data directory')\n",
    "    parser.add_argument('--file_name', type=str, metavar='<file_name>', help='File name to save model')\n",
    "    parser.add_argument('--small_part', type=int, default=0, help='Use part of training data')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='Training batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=50, help='Training epochs')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, help='Learing rate')\n",
    "\n",
    "    parser.add_argument('--input_dim', type=int, default=76, help='Dimension of visit record data')\n",
    "    parser.add_argument('--rnn_dim', type=int, default=384, help='Dimension of hidden units in RNN')\n",
    "    parser.add_argument('--output_dim', type=int, default=1, help='Dimension of prediction target')\n",
    "    parser.add_argument('--dropout_rate', type=float, default=0.5, help='Dropout rate')\n",
    "    parser.add_argument('--r_visit', type=int, default=4, help='Compress ration r for visit features')\n",
    "    parser.add_argument('--r_conv', type=int, default=4, help='Compress ration r for convolutional features')\n",
    "    parser.add_argument('--kernel_size', type=int, default=2, help='Convolutional kernel size')\n",
    "    parser.add_argument('--kernel_num', type=int, default=64, help='Number of convolutional filters')\n",
    "    parser.add_argument('--activation_func', type=str, default='sigmoid', help='Activation function for feature recalibration (sigmoid / sparsemax)')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    args = parse_arguments(parser)\n",
    "\n",
    "    if args.test_mode == 1:\n",
    "        print('Preparing test data ... ')\n",
    "\n",
    "        train_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(\n",
    "            args.data_path, 'train'), listfile=os.path.join(args.data_path, 'train_listfile.csv'), small_part=True)\n",
    "        discretizer = Discretizer(timestep=1.0, store_masks=True,\n",
    "                                impute_strategy='previous', start_time='zero')\n",
    "\n",
    "        discretizer_header = discretizer.transform(train_data_loader._data[\"X\"][0])[1].split(',')\n",
    "        cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "\n",
    "        normalizer = Normalizer(fields=cont_channels)\n",
    "        normalizer_state = 'decomp_normalizer'\n",
    "        normalizer_state = os.path.join(os.path.dirname(args.data_path), normalizer_state)\n",
    "        normalizer.load_params(normalizer_state)\n",
    "\n",
    "        test_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(args.data_path, 'test'),\n",
    "                                                                        listfile=os.path.join(args.data_path, 'test_listfile.csv'), small_part=args.small_part)\n",
    "        test_data_gen = utils.BatchGenDeepSupervision(test_data_loader, discretizer,\n",
    "                                                    normalizer, args.batch_size,\n",
    "                                                    shuffle=False, return_names=True)\n",
    "\n",
    "        print('Constructing model ... ')\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu')\n",
    "        print(\"available device: {}\".format(device))\n",
    "\n",
    "        model = AdaCare(device=device).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        \n",
    "        if torch.cuda.is_available() == True:\n",
    "            checkpoint = torch.load('./saved_weights/'+args.file_name)\n",
    "        else:\n",
    "            checkpoint = torch.load('./saved_weights/'+args.file_name, map_location=torch.device('cpu'))\n",
    "        save_chunk = checkpoint['chunk']\n",
    "        print(\"last saved model is in chunk {}\".format(save_chunk))\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            cur_test_loss = []\n",
    "            test_true = []\n",
    "            test_pred = []\n",
    "            \n",
    "            for each_batch in range(test_data_gen.steps):\n",
    "                test_data = next(test_data_gen)\n",
    "                test_name = test_data['names']\n",
    "                test_data = test_data['data']\n",
    "\n",
    "                test_x = torch.tensor(test_data[0][0], dtype=torch.float32).to(device)\n",
    "                test_mask = torch.tensor(test_data[0][1], dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "                test_y = torch.tensor(test_data[1], dtype=torch.float32).to(device)\n",
    "                \n",
    "                if test_mask.size()[1] > 400:\n",
    "                    test_x = test_x[:, :400, :]\n",
    "                    test_mask = test_mask[:, :400, :]\n",
    "                    test_y = test_y[:, :400, :]\n",
    "                \n",
    "                test_output, test_att = model(test_x, device)\n",
    "                masked_test_output = test_output * test_mask\n",
    "\n",
    "                test_loss = test_y * torch.log(masked_test_output + 1e-7) + (1 - test_y) * torch.log(1 - masked_test_output + 1e-7)\n",
    "                test_loss = torch.sum(test_loss, dim=1) / torch.sum(test_mask, dim=1)\n",
    "                test_loss = torch.neg(torch.sum(test_loss))\n",
    "                cur_test_loss.append(test_loss.cpu().detach().numpy()) \n",
    "                \n",
    "                for m, t, p in zip(test_mask.cpu().numpy().flatten(), test_y.cpu().numpy().flatten(), test_output.cpu().detach().numpy().flatten()):\n",
    "                    if np.equal(m, 1):\n",
    "                        test_true.append(t)\n",
    "                        test_pred.append(p)\n",
    "            \n",
    "            print('Test loss = %.4f'%(np.mean(np.array(cur_test_loss))))\n",
    "            print('\\n')\n",
    "            test_pred = np.array(test_pred)\n",
    "            test_pred = np.stack([1 - test_pred, test_pred], axis=1)\n",
    "            test_ret = metrics.print_metrics_binary(test_true, test_pred)\n",
    "\n",
    "    else:\n",
    "        ''' Prepare training data'''\n",
    "        print('Preparing training data ... ')\n",
    "        train_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(\n",
    "            args.data_path, 'train'), listfile=os.path.join(args.data_path, 'train_listfile.csv'), small_part=args.small_part)\n",
    "        val_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(\n",
    "            args.data_path, 'train'), listfile=os.path.join(args.data_path, 'val_listfile.csv'), small_part=args.small_part)\n",
    "        discretizer = Discretizer(timestep=1.0, store_masks=True,\n",
    "                                impute_strategy='previous', start_time='zero')\n",
    "\n",
    "        discretizer_header = discretizer.transform(train_data_loader._data[\"X\"][0])[1].split(',')\n",
    "        cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "\n",
    "        normalizer = Normalizer(fields=cont_channels)\n",
    "        normalizer_state = 'decomp_normalizer'\n",
    "        normalizer_state = os.path.join(os.path.dirname(args.data_path), normalizer_state)\n",
    "        normalizer.load_params(normalizer_state)\n",
    "\n",
    "        train_data_gen = utils.BatchGenDeepSupervision(train_data_loader, discretizer,\n",
    "                                                        normalizer, args.batch_size, shuffle=True, return_names=True)\n",
    "        val_data_gen = utils.BatchGenDeepSupervision(val_data_loader, discretizer,\n",
    "                                                    normalizer, args.batch_size, shuffle=False, return_names=True)\n",
    "\n",
    "        '''Model structure'''\n",
    "        print('Constructing model ... ')\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu')\n",
    "        print(\"available device: {}\".format(device))\n",
    "\n",
    "        model = AdaCare(args.rnn_dim, args.kernel_size, args.kernel_num, args.input_dim, args.output_dim, args.dropout_rate, args.r_visit, args.r_conv, args.activation_func, device).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "        '''Train phase'''\n",
    "        print('Start training ... ')\n",
    "\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        batch_loss = []\n",
    "        max_auprc = 0\n",
    "\n",
    "        file_name = './saved_weights/'+args.file_name\n",
    "        # file_name = './saved_weights/'+'second_run'\n",
    "        for each_chunk in range(args.epochs):\n",
    "            cur_batch_loss = []\n",
    "            model.train()\n",
    "            for each_batch in range(train_data_gen.steps):\n",
    "                batch_data = next(train_data_gen)\n",
    "                batch_name = batch_data['names']\n",
    "                batch_data = batch_data['data']\n",
    "\n",
    "                batch_x = torch.tensor(batch_data[0][0], dtype=torch.float32).to(device)\n",
    "                batch_mask = torch.tensor(batch_data[0][1], dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "                batch_y = torch.tensor(batch_data[1], dtype=torch.float32).to(device)\n",
    "               \n",
    "                if batch_mask.size()[1] > 400:\n",
    "                    batch_x = batch_x[:, :400, :]\n",
    "                    batch_mask = batch_mask[:, :400, :]\n",
    "                    batch_y = batch_y[:, :400, :]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                cur_output, _ = model(batch_x, device)\n",
    "                masked_output = cur_output * batch_mask \n",
    "                loss = batch_y * torch.log(masked_output + 1e-7) + (1 - batch_y) * torch.log(1 - masked_output + 1e-7)\n",
    "                loss = torch.sum(loss, dim=1) / torch.sum(batch_mask, dim=1)\n",
    "                loss = torch.neg(torch.sum(loss))\n",
    "                cur_batch_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if each_batch % 50 == 0:\n",
    "                    print('Chunk %d, Batch %d: Loss = %.4f'%(each_chunk, each_batch, cur_batch_loss[-1]))\n",
    "\n",
    "            batch_loss.append(cur_batch_loss)\n",
    "            train_loss.append(np.mean(np.array(cur_batch_loss)))\n",
    "            \n",
    "            print(\"\\n==>Predicting on validation\")\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                cur_val_loss = []\n",
    "                valid_true = []\n",
    "                valid_pred = []\n",
    "                for each_batch in range(val_data_gen.steps):\n",
    "                    valid_data = next(val_data_gen)\n",
    "                    valid_name = valid_data['names']\n",
    "                    valid_data = valid_data['data']\n",
    "                    \n",
    "                    valid_x = torch.tensor(valid_data[0][0], dtype=torch.float32).to(device)\n",
    "                    valid_mask = torch.tensor(valid_data[0][1], dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "                    valid_y = torch.tensor(valid_data[1], dtype=torch.float32).to(device)\n",
    "                    \n",
    "                    if valid_mask.size()[1] > 400:\n",
    "                        valid_x = valid_x[:, :400, :]\n",
    "                        valid_mask = valid_mask[:, :400, :]\n",
    "                        valid_y = valid_y[:, :400, :]\n",
    "                    \n",
    "                    valid_output, valid_dis = model(valid_x, device)\n",
    "                    masked_valid_output = valid_output * valid_mask\n",
    "\n",
    "                    valid_loss = valid_y * torch.log(masked_valid_output + 1e-7) + (1 - valid_y) * torch.log(1 - masked_valid_output + 1e-7)\n",
    "                    valid_loss = torch.sum(valid_loss, dim=1) / torch.sum(valid_mask, dim=1)\n",
    "                    valid_loss = torch.neg(torch.sum(valid_loss))\n",
    "                    cur_val_loss.append(valid_loss.cpu().detach().numpy())\n",
    "\n",
    "                    for m, t, p in zip(valid_mask.cpu().numpy().flatten(), valid_y.cpu().numpy().flatten(), valid_output.cpu().detach().numpy().flatten()):\n",
    "                        if np.equal(m, 1):\n",
    "                            valid_true.append(t)\n",
    "                            valid_pred.append(p)\n",
    "\n",
    "                val_loss.append(np.mean(np.array(cur_val_loss)))\n",
    "                print('Valid loss = %.4f'%(val_loss[-1]))\n",
    "                print('\\n')\n",
    "                valid_pred = np.array(valid_pred)\n",
    "                valid_pred = np.stack([1 - valid_pred, valid_pred], axis=1)\n",
    "                ret = metrics.print_metrics_binary(valid_true, valid_pred)\n",
    "                print()\n",
    "\n",
    "                cur_auprc = ret['auprc']\n",
    "                if cur_auprc > max_auprc:\n",
    "                    max_auprc = cur_auprc\n",
    "                    state = {\n",
    "                        'net': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'chunk': each_chunk\n",
    "                    }\n",
    "                    torch.save(state, file_name)\n",
    "                    print('\\n------------ Save best model ------------\\n')\n",
    "\n",
    "\n",
    "        '''Evaluate phase'''\n",
    "        print('Testing model ... ')\n",
    "\n",
    "        checkpoint = torch.load(file_name)\n",
    "        save_chunk = checkpoint['chunk']\n",
    "        print(\"last saved model is in chunk {}\".format(save_chunk))\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        model.eval()\n",
    "\n",
    "        test_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(args.data_path, 'test'),\n",
    "                                                                        listfile=os.path.join(args.data_path, 'test_listfile.csv'), small_part=args.small_part)\n",
    "        test_data_gen = utils.BatchGenDeepSupervision(test_data_loader, discretizer,\n",
    "                                                    normalizer, args.batch_size,\n",
    "                                                    shuffle=False, return_names=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            torch.manual_seed(RANDOM_SEED)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "        \n",
    "            cur_test_loss = []\n",
    "            test_true = []\n",
    "            test_pred = []\n",
    "            \n",
    "            for each_batch in range(test_data_gen.steps):\n",
    "                test_data = next(test_data_gen)\n",
    "                test_name = test_data['names']\n",
    "                test_data = test_data['data']\n",
    "\n",
    "                test_x = torch.tensor(test_data[0][0], dtype=torch.float32).to(device)\n",
    "                test_mask = torch.tensor(test_data[0][1], dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "                test_y = torch.tensor(test_data[1], dtype=torch.float32).to(device)\n",
    "                \n",
    "                if test_mask.size()[1] > 400:\n",
    "                    test_x = test_x[:, :400, :]\n",
    "                    test_mask = test_mask[:, :400, :]\n",
    "                    test_y = test_y[:, :400, :]\n",
    "\n",
    "                test_output, test_dis = model(test_x, device)\n",
    "                masked_test_output = test_output * test_mask\n",
    "\n",
    "                test_loss = test_y * torch.log(masked_test_output + 1e-7) + (1 - test_y) * torch.log(1 - masked_test_output + 1e-7)\n",
    "                test_loss = torch.sum(test_loss, dim=1) / torch.sum(test_mask, dim=1)\n",
    "                test_loss = torch.neg(torch.sum(test_loss))\n",
    "                cur_test_loss.append(test_loss.cpu().detach().numpy()) \n",
    "                \n",
    "                for m, t, p in zip(test_mask.cpu().numpy().flatten(), test_y.cpu().numpy().flatten(), test_output.cpu().detach().numpy().flatten()):\n",
    "                    if np.equal(m, 1):\n",
    "                        test_true.append(t)\n",
    "                        test_pred.append(p)\n",
    "            \n",
    "            print('Test loss = %.4f'%(np.mean(np.array(cur_test_loss))))\n",
    "            print('\\n')\n",
    "            test_pred = np.array(test_pred)\n",
    "            test_pred = np.stack([1 - test_pred, test_pred], axis=1)\n",
    "            test_ret = metrics.print_metrics_binary(test_true, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
